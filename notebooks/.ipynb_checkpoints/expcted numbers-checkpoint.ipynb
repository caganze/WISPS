{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 89 sources from /Users/caganze/research/splat//resources/Spectra/Public/MAGE/ to spectral database\n",
      "Adding 145 sources from /Users/caganze/research/splat//resources/Spectra/Public/LRIS-RED/ to spectral database\n",
      "Adding 2404 sources from /Users/caganze/research/splat//resources/Spectra/Public/SPEX-PRISM/ to spectral database\n",
      "\n",
      "Warning: spectrum object has a flux vector of zero length - maybe empty?\n",
      "\n",
      "Warning: normalize is attempting to divide by nan; ignoring\n",
      "\n",
      "Warning: spectrum object has a flux vector of zero length - maybe empty?\n",
      "\n",
      "Warning: normalize is attempting to divide by nan; ignoring\n",
      "\n",
      "Warning: spectrum object has a flux vector of zero length - maybe empty?\n",
      "\n",
      "Warning: normalize is attempting to divide by nan; ignoring\n",
      "\n",
      "Warning: spectrum object has a flux vector of zero length - maybe empty?\n",
      "\n",
      "Warning: normalize is attempting to divide by nan; ignoring\n",
      "\n",
      "Warning: spectrum object has a flux vector of zero length - maybe empty?\n",
      "\n",
      "Warning: normalize is attempting to divide by nan; ignoring\n"
     ]
    }
   ],
   "source": [
    "import wisps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wisps.simulations as wispsim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "import astropy.units as u \n",
    "import wisps.simulations.effective_numbers as eff\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import itertools\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrid=wispsim.SPGRID\n",
    "pnts=pd.read_pickle(wisps.OUTPUT_FILES+'/pointings_correctedf110.pkl')\n",
    "corr_pols=wisps.POLYNOMIAL_RELATIONS['mag_limit_corrections'] \n",
    "klf=pd.read_csv('/users/caganze/research/wisps/data/kirkpatricklf.txt', delimiter=',')\n",
    "klf['bin_center']=np.mean(np.array([klf.t0.values, klf.tf.values]), axis=0)\n",
    "klf=klf.replace(0.0,np.nan)\n",
    "\n",
    "cands=pd.read_pickle(wisps.LIBRARIES+'/real_ucds.pkl')\n",
    "cands=cands[(cands.spt >=17) & (cands.snr1>=3)].reset_index(drop=True)\n",
    "tab=wisps.Annotator.reformat_table(cands)\n",
    "pnt_names=[x.name for x in pnts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmap= sns.color_palette(\"coolwarm\", 8, as_cmap=True)\n",
    "cmap=matplotlib.cm.get_cmap('coolwarm')\n",
    "cnorm=Normalize(wispsim.HS[0]/100, (wispsim.HS[-1])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kirkpatrick2020LF={'bin_center':np.flip(np.array([2025, 1875, 1725, 1575, 1425, 1275, 1125 , 975, 825, 675, 525])), \n",
    "                   'values':np.flip(np.array([0.72, 0.50,0.78, 0.81,0.94, 1.95, 1.11, 1.72, 1.99, 2.80, 4.24])), \n",
    "                   'unc':np.flip(([0.18, 0.17, 0.20,0.20, 0.22, 0.3, 0.25, 0.3, 0.32, 0.37, 0.70]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_by_spt_bin(sp_types, number, ltonly=False):\n",
    "    ranges=[[17, 20], [20, 25], [25, 30], [30, 35], [35, 40]]\n",
    "    if ltonly:\n",
    "        ranges=[[17, 20], [20, 30], [30, 41]]\n",
    "    numbers=[]\n",
    "    for r in ranges:\n",
    "        idx= np.logical_and((r[0]<=sp_types), (r[1]>sp_types))\n",
    "        numbers.append(np.nansum(number[idx]))\n",
    "    return numbers\n",
    "\n",
    "def get_all_numbers():\n",
    "    #Distribute the parameter sets evenly across the cores\n",
    "    func=lambda x, y:  get_simulated_number_model(y, x)\n",
    "\n",
    "    paramlist=[(i, j)  for i, j in itertools.product(['saumon2008', 'baraffe2003', 'marley2019', 'phillips2020'], wispsim.HS)]\n",
    "    res  = [func(x, y) for x,y in tqdm(paramlist)]\n",
    "    \n",
    "    nbrs = {}\n",
    "    for k in ['saumon2008', 'marley2019', 'phillips2020', 'baraffe2003']:\n",
    "        ds0={}\n",
    "        for j in res:\n",
    "            if k in j.keys():\n",
    "                key=[x for x in j[k].keys()][0]\n",
    "                ds0.update({key: [(j[k][key])[yi] for yi in wispsim.SPGRID]})\n",
    "        #print (ds0)\n",
    "        nbrs[k]=np.array([ds0[k] for k in wispsim.HS])\n",
    "\n",
    "    return nbrs\n",
    "    \n",
    "\n",
    "\n",
    "def get_pointing(grism_id):\n",
    "    if grism_id.startswith('par'):\n",
    "        pntname=grism_id.lower().split('-')[0]\n",
    "    else:\n",
    "        pntname=grism_id.lower().split('-g141')[0]\n",
    "    loc=pnt_names.index(pntname)\n",
    "    return np.array(pnts)[loc]\n",
    "\n",
    "\n",
    "def iswithin_mag_limits(mags, pnt, spt):\n",
    "    #mgs is a dictionary\n",
    "    flags=[]\n",
    "    for k in pnt.mag_limits.keys():\n",
    "        if k =='F110' and pnt.survey =='hst3d':\n",
    "            flags.append(True)\n",
    "        else:\n",
    "            flags.append(mags[k] <= pnt.mag_limits[k]+ (corr_pols['F110W'][0]+1.5)(spt))\n",
    "    return np.logical_or.reduce(flags)\n",
    "\n",
    "def scale_lf_teff(teffs):\n",
    "    binedges= np.append(kirkpatrick2020LF['bin_center']-75, kirkpatrick2020LF['bin_center'][-1]-75)\n",
    "    bools=np.logical_and(teffs <= binedges[-1], teffs >= binedges[0])\n",
    "    preds=np.histogram(teffs, bins=binedges, normed=False)[0]\n",
    "    \n",
    "    obs=np.array(kirkpatrick2020LF['values'])\n",
    "    unc=np.array(kirkpatrick2020LF['unc'])\n",
    "    \n",
    "    obs_monte_carlo= np.random.normal(obs, unc, (10000, len(obs)))\n",
    "    pred_monte= np.ones_like(obs_monte_carlo)*(preds)\n",
    "    unc_monte=  np.ones_like(obs_monte_carlo)*(unc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #unc_monte= \n",
    "    scale=(np.nansum((obs_monte_carlo*pred_monte)/(unc_monte**2), axis=1)\\\n",
    "           /np.nansum(((pred_monte**2)/(unc_monte**2)), axis=1))*(10**-3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #fig, ax=plt.subplots()\n",
    "    #ax.step(kirkpatrick2020LF['bin_centertw'], preds*scale*(10**-3), where='mid')\n",
    "    #ax.errorbar(kirkpatrick2020LF['bin_center'], np.array(kirkpatrick2020LF['values'])*(10**-3),\n",
    "    #         yerr= np.array(kirkpatrick2020LF['unc'])*(10**-3), fmt='o', color='#111111')\n",
    "    return [np.nanmean(scale), np.nanstd(scale), len(teffs)]\n",
    "def get_simulated_number_model(hidx, model):\n",
    "    #hidx is a scale height, model is evolutionary model\n",
    "    cutdf=pd.read_hdf(wisps.OUTPUT_FILES+'/final_simulated_sample_cut.h5', key=str(model)+str('h')+str(hidx)+'F110_corrected')\n",
    "    scl_dict=pd.read_pickle(wisps.OUTPUT_FILES+'/lf_scales.pkl') \n",
    "    scale=scl_dict[model]\n",
    "    #scale=scale_lf_teff(cutdf.teff)\n",
    "    NSIM=dict(zip(wispsim.SPGRID,np.zeros((len(wispsim.SPGRID), 2))))\n",
    "    cutdf['spt_r']=cutdf.spt.apply(np.round)\n",
    "    for g in cutdf.groupby('spt_r'):\n",
    "        sn= len(cutdf.teff[np.logical_and(cutdf.teff>=450, cutdf.teff<=1950)])\n",
    "        scln=np.array([scale[-1]/sn, (scale[1]*scale[-1])/(sn*scale[0])])\n",
    "        #scln=np.array(scale)\n",
    "        #assert scln[0] > scale[0]\n",
    "        NSIM[g[0]]=np.nansum(g[1].sl)*scln\n",
    "    del cutdf\n",
    "    return {model: {hidx:NSIM}}\n",
    "\n",
    "\n",
    "def plot(NUMBERS, VOLUMES, filename='/oberved_numbers.pdf'):\n",
    "    # In[ ]:\n",
    "    nall=wisps.custom_histogram(cands.spt.apply(wisps.make_spt_number), sgrid, 1)\n",
    "    \n",
    "    y2=bin_by_spt_bin(wispsim.SPGRID,nobs, ltonly=False)\n",
    "    yall=bin_by_spt_bin(wispsim.SPGRID,nall, ltonly=False)\n",
    "    \n",
    "    dy2=np.sqrt(y2)\n",
    "    dyall=np.sqrt(yall)\n",
    "\n",
    "    fig, ax=plt.subplots(figsize=(12, 12), ncols=2, nrows=2, sharey=False, sharex=False)\n",
    "    \n",
    "    for model, a in zip(['baraffe2003', 'saumon2008', 'marley2019', 'phillips2020'], np.concatenate(ax)):\n",
    "        \n",
    "        for idx, h in enumerate(wispsim.HS):\n",
    "            \n",
    "            ns=None\n",
    "            ns=((NUMBERS[model])[idx])[:,0]*VOLUMES[idx]\n",
    "            nuncs=((NUMBERS[model])[idx])[:,1]*VOLUMES[idx]\n",
    "            \n",
    "            a.plot(spgrid2, bin_by_spt_bin(wispsim.SPGRID,ns, ltonly=False), \n",
    "                          label='h={} pc'.format(h), color= cmap(cnorm(h/100)), \n",
    "                   linewidth=3, drawstyle=\"steps-mid\")\n",
    "            a.fill_between(spgrid2, bin_by_spt_bin(wispsim.SPGRID,ns+nuncs, ltonly=False),  \n",
    "                           bin_by_spt_bin(wispsim.SPGRID,ns-nuncs, ltonly=False), alpha=0.5, \n",
    "                           color= cmap(cnorm(h/100)),  step=\"mid\")\n",
    "        \n",
    "        a.set_yscale('log')\n",
    "        a.errorbar(spgrid2,y2, yerr=dy2,fmt='o', color='#111111')\n",
    "        a.errorbar(spgrid2,yall, yerr=dyall,color='#B10DC9', fmt='o')\n",
    "        a.set_xlabel('SpT',fontsize=18)\n",
    "        a.set_ylabel('N',fontsize=18)\n",
    "        a.minorticks_on()\n",
    "            \n",
    "\n",
    "\n",
    "    ax[0][1].set_title('Model= SM08', fontsize=18)\n",
    "    ax[0][0].set_title('Model= B03', fontsize=18)\n",
    "    ax[1][0].set_title('Model= M19', fontsize=18)\n",
    "    ax[1][1].set_title('Model= P20', fontsize=18)\n",
    "\n",
    "    ax[0][0].errorbar(spgrid2,y2, yerr=dy2,fmt='o', label='observations', color='#111111')\n",
    "    ax[0][0].errorbar(spgrid2,yall, yerr=dyall,color='#B10DC9', fmt='o', label='All observations')\n",
    "    \n",
    "    ax[0][0].legend(fontsize=14, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(wisps.OUTPUT_FIGURES+filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_simulated_number_model(350, 'saumon2008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d=pd.read_pickle(wisps.OUTPUT_FILES+'/distance_samples{}'.format(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(d[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-20605668b981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pnt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grism_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pointing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spt_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m obsmgs=tab[['F140W', 'F110W', 'F160W']].rename(columns={\"F110W\": \"F110\", \n\u001b[1;32m      4\u001b[0m                                                                     \u001b[0;34m\"F140W\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"F140\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                     \"F160W\": \"F160\"}).to_dict('records')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tab' is not defined"
     ]
    }
   ],
   "source": [
    "tab['pnt']=tab['grism_id'].apply(get_pointing)\n",
    "tab['spt_val']=np.vstack(tab.spt.values)[:,0]\n",
    "obsmgs=tab[['F140W', 'F110W', 'F160W']].rename(columns={\"F110W\": \"F110\", \n",
    "                                                                    \"F140W\": \"F140\",\n",
    "                                                                    \"F160W\": \"F160\"}).to_dict('records')\n",
    "\n",
    "flags=[iswithin_mag_limits(x, y, z) for x, y, z in zip(obsmgs, tab.pnt.values,tab.spt.values )]\n",
    "\n",
    "#let's see what happens if we include all objects\n",
    "#flags=np.ones(len(flags)).astype(bool)\n",
    "cdf_to_use=tab[flags]\n",
    "\n",
    "nobs=wisps.custom_histogram(cdf_to_use.spt_val.apply(wisps.make_spt_number), sgrid, 1)\n",
    "\n",
    "spgrid2=['M7-L0', 'L0-L5', 'L5-T0', 'T0-T5', 'T5-Y0']\n",
    "spgrid3=['Late M', 'L', 'T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['F140', 'F110', 'F160']:\n",
    "    tab['lim_{}'.format(k)]=tab.pnt.apply(lambda x: x.mag_limits[k])\n",
    "    tab['detected_{}'.format(k)]= tab[k+'W'] < tab['lim_{}'.format(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtab=(tab[tab.spt.between(30, 35)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#    print( subtab[['F140W', 'F160W', 'lim_F140', 'lim_F160', 'detected_F140', 'detected_F160', 'grism_id',\n",
    "#                  'spt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBERS=pd.read_pickle(wisps.OUTPUT_FILES+'/numbers_simulated.pkl')\n",
    "NUMBERS=get_all_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(np.log10(NUMBERS['baraffe2003'][0][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes=[]\n",
    "for pnt in pnts:\n",
    "    vs=[]\n",
    "    for h in wispsim.HS:\n",
    "        vsx=[]\n",
    "        for g in wispsim.SPGRID:\n",
    "            vsx.append((pnt.volumes[h])[g])\n",
    "        vs.append(vsx)\n",
    "    volumes.append(vs)\n",
    "volumes=np.array(volumes)\n",
    "\n",
    "VOLUMES=(np.nansum(volumes, axis=0))*4.1*(u.arcmin**2).to(u.radian**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot(NUMBERS, VOLUMES, filename='/obs_numbers.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nall=wisps.custom_histogram(cands.spt.apply(wisps.make_spt_number), sgrid, 1)\n",
    "y2=bin_by_spt_bin(wispsim.SPGRID,nobs, ltonly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cands[cands.spt.between(30, 35)].F140W,cands[cands.spt.between(30, 35)].F160W, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_by_spt_bin(wispsim.SPGRID,nall, ltonly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for L dwarfs and T dwarfs\n",
    "y3=bin_by_spt_bin(wispsim.SPGRID,nall, ltonly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4=np.append(y2, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT THE BEST FIT NUMBER \n",
    "#best_fit={}\n",
    "numbers_fit={}\n",
    "for model in ['saumon2008', 'baraffe2003', 'marley2019', 'phillips2020']:\n",
    "        model_fit={}\n",
    "        model_number={}\n",
    "        for idx, h in enumerate(wispsim.HS):\n",
    "            \n",
    "            ns=None\n",
    "            ns=((NUMBERS[model])[idx])[:,0]*VOLUMES[idx]\n",
    "            nuncs=((NUMBERS[model])[idx])[:,1]*VOLUMES[idx]\n",
    "            \n",
    "            binned=np.array(bin_by_spt_bin(wispsim.SPGRID,ns, ltonly=False))\n",
    "            binned_unc=np.array(bin_by_spt_bin(wispsim.SPGRID,nuncs, ltonly=False))\n",
    "            #add L and \n",
    "            #compute chi-squared\n",
    "            #print (ns)\n",
    "            #chisq= abs((y2-binned)**2/(y2))\n",
    "            #model_fit.update({h: chisq})\n",
    "            #binned_total=np.append(binned, binned_lt)\n",
    "            #binned_total=np.append(binned, binned_lt)\n",
    "            model_number.update({h: binned})\n",
    "        # best_fit.update({model: model_fit})\n",
    "        numbers_fit.update({model: model_number})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chisq_dicts=pd.DataFrame.from_records(best_fit)\n",
    "number_dicts=pd.DataFrame.from_records(numbers_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tab[tab.spt.between(30, 35)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(10, 6))\n",
    "idx=0\n",
    "for c in number_dicts.columns:\n",
    "    \n",
    "    vals=np.vstack([number_dicts[c][h] for h in wispsim.HS])[:,idx]\n",
    "\n",
    "    interpf = interp1d(vals, wispsim.HS)\n",
    "    npoisson=np.random.poisson(y2[idx], 100000).astype(float)\n",
    "    #assign up and down \n",
    "    dflag=npoisson>=vals.min()\n",
    "    uflag= npoisson <=vals.max()\n",
    "    npoisson[dflag]= vals.min()\n",
    "    npoisson[uflag]= vals.max()\n",
    "    predhs=interpf(npoisson)\n",
    "    sortarray=np.argsort(npoisson)\n",
    "    plt.plot(  (npoisson)[sortarray], predhs[sortarray],'o-', c='k', alpha=0.1)\n",
    "    plt.plot(vals, wispsim.HS, 'o-', label=c)\n",
    "plt.ylabel('h (expcted)', fontsize=18)\n",
    "plt.xlabel('N', fontsize=18)\n",
    "plt.xscale('log')\n",
    "plt.axvline(y3[idx], c='k')\n",
    "plt.legend()\n",
    "plt.minorticks_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wispsim.HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_dicts['marley2019'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_chi_ssqrs={}\n",
    "for c in number_dicts.columns:\n",
    "    min_vals={}\n",
    "    #for idx,s  in enumerate(np.append(spgrid2, ['L dwarfs', 'T dwarfs'])):\n",
    "    for idx,s  in enumerate(spgrid2):\n",
    "        #compare between subtypes\n",
    "        #predicted\n",
    "        vals=(np.vstack(number_dicts[c].values))[:,idx]\n",
    "        #observed\n",
    "        nreal= y3[idx]\n",
    "        #make an interpolation function\n",
    "        interpf = interp1d(vals, wispsim.HS)\n",
    "        #using a 2nd degree polynomial \n",
    "        \n",
    "        #draw a bunch of random values based on a poisson distribution\n",
    "        npoisson=np.random.poisson(nreal, 100000).astype(float)\n",
    "        #stay within the range of possible values to avoid interpolation error\n",
    "        #i.e take this as a prior\n",
    "        #dflag=npoisson>=vals.min()\n",
    "        #uflag= npoisson <=vals.max()\n",
    "        #npoisson[dflag]= vals.min()\n",
    "        #npoisson[uflag]= vals.max()\n",
    "        npoisson=npoisson[np.logical_and(npoisson>=vals.min(), npoisson <=vals.max())]\n",
    "        #predict scale heights\n",
    "        predhs=interpf(npoisson)\n",
    "        #use a weighted mean and std \n",
    "        mean, unc= (np.nanmean(predhs), np.nanstd(predhs))\n",
    "    \n",
    "        \n",
    "        min_vals.update({s:[np.round(mean), np.round(unc, 4)]})\n",
    "        #final minimal\n",
    "        print (' scale height for model {} and spt {} is {} +/- {} '.format(c, s, np.round(mean), np.round(unc, 4)))\n",
    "        \n",
    "    min_chi_ssqrs.update({c:min_vals})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_uncertainties(row):\n",
    "    vals=np.vstack(row.values)[:, 0]\n",
    "    uncs=np.vstack(row.values)[:,1]\n",
    "    \n",
    "    return round(np.nanmean(vals)), round(np.sqrt(np.nansum(uncs**2)+ np.nanstd(vals)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_chi_ssqrs_df=pd.DataFrame(min_chi_ssqrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't use certain models for certain types\n",
    "min_chi_ssqrs_df.phillips2020['M7-L0']=[np.nan, np.nan]\n",
    "min_chi_ssqrs_df.marley2019['M7-L0']=[np.nan, np.nan]\n",
    "min_chi_ssqrs_df.saumon2008['M7-L0']=[np.nan, np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_chi_ssqrs_df.applymap(lambda x: np.round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_chi_ssqrs_df.apply(propagate_uncertainties, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use my velocity dispersion converter \n",
    "def velocity_dispersion(h, hunc):\n",
    "    #do a monte carlo uncertainty prop\n",
    "    hs= np.random.normal(h, hunc, 10000)\n",
    "    vals=np.sqrt((hs*68)/435*20)\n",
    "    return np.nanmean(vals), np.nanstd(vals)\n",
    "\n",
    "vel_tables=min_chi_ssqrs_df.applymap(lambda x: velocity_dispersion(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do monte carlo uncertainty propagation\n",
    "def get_age(sigma, sigmaunc, beta, tau1, sigma10):\n",
    "    sigmas=np.random.normal(sigma, sigmaunc, 10000)\n",
    "    vals=((sigmas/sigma10)**(1/beta))*(10+tau1)-tau1\n",
    "    return np.nanmean(vals), np.nanstd(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tables= vel_tables.applymap(lambda x: get_age(x[0], x[1], 0.385, 0.261, 57.157))\n",
    "#age_tables=age_tables.applymap(lambda x:np.array(x)).apply(lambda x: np.round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age_emp=pd.DataFrame()\n",
    "#age_emp['subtype']=np.array(age_tables.index)\n",
    "#age_emp['age']=np.vstack(age_tables.values)[:,0]\n",
    "#age_emp['unc']=np.vstack(age_tables.values)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age_tables.apply(propagate_uncertainties, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upper and lo limits on ages \n",
    "up_lims_table=pd.DataFrame(columns= age_tables.columns,\n",
    "                           index=age_tables.index).fillna(0)\n",
    "up_lims_table.saumon2008['T0-T5']=1\n",
    "up_lims_table.saumon2008['T0-T5']=1\n",
    "\n",
    "#lo limts\n",
    "lo_lims_table=pd.DataFrame(columns= age_tables.columns,\n",
    "                           index=age_tables.index).fillna(0)\n",
    "lo_lims_table.baraffe2003['T5-Y0']=1\n",
    "lo_lims_table.baraffe2003['L5-T0']=1\n",
    "lo_lims_table.phillips2020['T5-Y0']=1\n",
    "lo_lims_table.saumon2008['T5-Y0']=1\n",
    "lo_lims_table.marley2019['T5-Y0']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_lims_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simpler_class(x):\n",
    "    if x.startswith('M'):\n",
    "        return 'Late M'\n",
    "    if x.startswith('L'):\n",
    "        return 'L'\n",
    "    if x.startswith('T'):\n",
    "        return 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot age with scale heights\n",
    "age_dictionaries={}\n",
    "for model in ['saumon2008', 'baraffe2003', 'marley2019', 'phillips2020']:\n",
    "    dfs=[]\n",
    "    for hidx in wispsim.HS:\n",
    "        dfs.append(pd.read_hdf(wisps.OUTPUT_FILES+'/final_simulated_sample_cut.h5',\n",
    "                               key=str(model)+str('h')+str(hidx)+'F110_corrected'))\n",
    "        \n",
    "    df=pd.concat(dfs)\n",
    "    print (len(df))\n",
    "    cutdf_lblded=wisps.Annotator.group_by_spt(df, spt_label='spt', assign_number=False).rename(columns={'spt_range': 'subtype'})\n",
    "    cutdf_lblded['spectclass']=  cutdf_lblded.subtype.apply(get_simpler_class)\n",
    "    final_df=cutdf_lblded[~((cutdf_lblded.spectclass=='') | (cutdf_lblded.subtype=='')|   (cutdf_lblded.subtype=='trash'))]\n",
    "    age_dictionaries[model]=final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax=plt.subplots(figsize=(12, 12), ncols=2, nrows=2, sharex=False, sharey=False)\n",
    "\n",
    "for model, a in zip([ 'baraffe2003', 'saumon2008','marley2019', 'phillips2020'], np.concatenate(ax)):\n",
    "    dfn=age_dictionaries[model].replace('T5-T9', 'T5-Y0')\n",
    "    dfn['Survey']=dfn.pnt.apply(lambda x: x.survey)\n",
    "    dfn.Survey=dfn.Survey.replace('hst3d', '3d-hst'.upper())\n",
    "    dfn.Survey=dfn.Survey.replace('wisps', 'wisps'.upper())\n",
    "    age_emp=pd.DataFrame()\n",
    "    age_emp['subtype']=np.array(age_tables[model].index)\n",
    "    age_emp['age']=np.vstack(age_tables[model].values)[:,0]\n",
    "    age_emp['unc']=np.vstack(age_tables[model].values)[:,1]\n",
    "    age_emp.unc[age_emp.unc<0.1]=1\n",
    "    agfn= age_emp\n",
    "    #lower limits\n",
    "    lolims=lo_lims_table[model].values.astype(bool)\n",
    "    #upper limits\n",
    "    uplims =up_lims_table[model].values.astype(bool)\n",
    "    if model !='baraffe2003':\n",
    "        dfn=dfn[dfn.subtype != 'M7-L0']\n",
    "        agfn=agfn[agfn.subtype !='M7-L0']\n",
    "        lolims=lo_lims_table[model].values.astype(bool)[1:]\n",
    "        uplims =up_lims_table[model].values.astype(bool)[1:]\n",
    "\n",
    "        sns.violinplot(x='age', y='subtype', data=dfn, ax=a,\n",
    "                    palette=\"tab20c\", hue='Survey', saturation=0.9, scale='area',\n",
    "                       order=['L0-L5', 'L5-T0', 'T0-T5', 'T5-Y0'], \n",
    "                       split=True, linewidth=0.5, inner=\"quartile\")\n",
    "        a.set_ylim([-0.5, 3.5])\n",
    "    else:\n",
    "        sns.violinplot(x='age', y='subtype', data=dfn, ax=a,\n",
    "                    palette=\"tab20c\", hue='Survey', saturation=0.9, scale='area', \n",
    "                       order=spgrid2, split=True, linewidth=0.5, inner=\"quartile\")\n",
    "        a.set_ylim([-0.5, 4.5])\n",
    "        \n",
    "    \n",
    "    #sns.boxplot(y=dfn.age.values*dfn.slprob, x=dfn.spectclass, ax=a,\n",
    "    #            color='#FF851B')\n",
    "    \n",
    "    #upper and low limit of the error\n",
    "    #print (lolims =\n",
    "    a.errorbar(  agfn.age, agfn.subtype, xerr=agfn.unc,  fmt='o',xlolims=lolims, ms=20, lw=7, capsize=7, \n",
    "               mfc='#FF4136', mec='#111111', ecolor='#111111', xuplims=uplims)\n",
    "    #a.set_xlim()\n",
    "    a.set_ylabel('Subtype Group', fontsize=18)\n",
    "    a.set_xlabel('Age (Gyr)', fontsize=18)\n",
    "    a.set_xlim([-1, 9.])\n",
    "    a.minorticks_on()\n",
    "\n",
    "ax[0][0].set_title('Model= B03', fontsize=18)\n",
    "ax[0][1].set_title('Model= SM08', fontsize=18)\n",
    "ax[1][0].set_title('Model= M19', fontsize=18)\n",
    "ax[1][1].set_title('Model= P20', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(wisps.OUTPUT_FIGURES+'/age_comparison.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wisps.LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
